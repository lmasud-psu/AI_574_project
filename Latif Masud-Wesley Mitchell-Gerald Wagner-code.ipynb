{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online banking fraud detection using natural language processing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team 9:**\n",
    "\n",
    "- Latif Masud\n",
    "- ​Wesley Mitchell\n",
    "- Gerald Wagner​\n",
    "\n",
    "**Course:** AI 574 – Natural Language Processing (Spring 2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "* This project aims to identify fraudulent activity in online banking transactions using Natural Language Processing techniques. Online banking activity can be monitored by the webpages or API endpoints a user interacts with throughout their entire session history. With this sequence of user actions, a binary classification can be trained such that it labels the activity as valid or fraudulent; if fraudulent, remediation steps could then be implemented such as denying the transaction. With online banking a staple of people every day financial lives and 100's of millions of dollars transacted daily, identifying fraudulent activity is of upmost importance to prevent unnecessary monetary losses for both individuals and financial institutions.\n",
    "    \n",
    "* **Keywords:** Online banking, fraud, fraud detection, financial industry "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "* Source(url): https://github.com/pboulieris/FraudNLP/blob/master/Fraud%20Detection%20with%20Natural%20Language%20Processing.rar\n",
    "* Short Description: The data set of 105,303 online banking transactions with 9 transaction characteristics:\n",
    "    * Action time mean: the average time between actions in a transaction\n",
    "    * Action time std: the standard deviation of the time between actions\n",
    "    * log(amount): the natural logarithm of the transaction amount\n",
    "    * Transaction Type: a string indicating whether the transaction is fraudulent or not\n",
    "    * time_to_first_action: the time between the start of the transaction and the first action taken\n",
    "    * actions_str: a string containing the names of all actions taken in the transaction\n",
    "    * total_time_to_transaction: the total time elapsed from the start of the transaction to its completion\n",
    "\n",
    "* Keywords: bank transactions, user actions, API endpoints, webpage urls, dollar amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "\n",
    "* the following packages are required to run this notebook:\n",
    "    * pandas\n",
    "    * scikit-learn\n",
    "\n",
    "Install by creating and activating a virtual environment, then installing via the pip command:\n",
    "\n",
    "!pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online banking transaction data\n",
    "path = Path('./data/Fraud Detection with Natural Language Processing.pkl')\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary of API calls\n",
    "path_vocab = Path('./data/vocab.csv')\n",
    "df_vocab = pd.read_csv(path_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('is_fraud').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA Summary\n",
    "\n",
    "- the transaction dataset contains 105303 online banking transactions\n",
    "- of the 105303 transactions, 105202 are valid while only 101 are fraudulent\n",
    "    - this is a severe class imbalance that will have to be handled in the neural network architecture\n",
    "- there are 9 attributes for each banking transaction:\n",
    "    - a label for trasactions that are valid or fraudulent (0 or 1 respectively)\n",
    "    - list of user actions encoded as a list of integers which corresponds to the vocabulary dataframe\n",
    "    - list of times in ms for each user action to occur\n",
    "    - the total elapsed time of the transaction in ms\n",
    "    - Recency, Frequency, and Monetary features:\n",
    "        - the transaction amount in log(Euros)\n",
    "        - the device characteristics\n",
    "        - the IP address of the user\n",
    "        - the beneficiary's frequency of conducting a transaction\n",
    "        - the applications used for the transaction (i.e., Android or iOS)\n",
    "- there also exists a vocabulary dataset which contains a list of API endpoints/webpage urls which a user can access\n",
    "    - these are used to translate the encoded user action column of the transaction dataset back to the original url's\n",
    "    - there are 1916 total endpoints/url's, all of which are unique\n",
    "    - the index of the dataframe corresponds to the id value used in the user action list from the transaction dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "* Enumerate and present the main steps you preformed in the data preprocessing\n",
    "* Add your code and interpret the outcome of main steps/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping ids in transaction data to vocabulary\n",
    "vocab = df_vocab['Name'].to_list()\n",
    "\n",
    "vocab_sentences = []\n",
    "for endpoint in vocab:\n",
    "    sentence = endpoint.replace('/', ' ').lstrip() + ' .'\n",
    "    vocab_sentences.append(sentence)\n",
    "\n",
    "id_to_action = {i:a for i, a in enumerate(vocab_sentences)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tokenized user actions during online banking to API endpoint calls\n",
    "actions_raw = df['actions'].to_list()\n",
    "\n",
    "actions = []\n",
    "for action in actions_raw:\n",
    "\n",
    "    action_str = (action.replace('[', '')\n",
    "           .replace(']', '')\n",
    "           .replace(' ', '')\n",
    "           .split(','))\n",
    "    \n",
    "    action_ids = []\n",
    "    for id in action_str:\n",
    "        if id:\n",
    "            \n",
    "            action_ids.append(id_to_action[int(id)])\n",
    "\n",
    "    actions.append(' '.join(action_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of labels\n",
    "labels = df['is_fraud'].to_list()\n",
    "\n",
    "print(f'there are {sum(labels)} fraudulent transactions')\n",
    "print(f'which is only {sum(labels)/len(labels)*100:0.2f}% of the total transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the data into training and testing datasets\n",
    "# enable the stratify option to ensure there are proportional amounts of fraudulent transactions in the training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(actions, labels, test_size=0.2, shuffle=True, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(y_train))\n",
    "print(sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "1. Explan your Deep Learning process / methodology\n",
    "\n",
    "\n",
    "\n",
    "2. Introduce the Deep Neural Networks you used in your project\n",
    " * Model 1\n",
    "    * Description \n",
    " \n",
    " * Model 2\n",
    "    * Description\n",
    " \n",
    " * Ensemble method\n",
    "     * Description \n",
    " \n",
    " \n",
    "3. Add keywords  \n",
    "**Keywords:** natural language processing, sentiment analysis, clustering, binary classification, multi-label classification, prediction\n",
    "\t___\n",
    " **Example**\n",
    "* ConvNet\n",
    "    * A convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery(source Wikipedia). \n",
    " \n",
    "* **Keywords:** supervised learning, classification, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Validation\n",
    "\n",
    "1. model 1 \n",
    "    - decription \n",
    "2. model 2\n",
    "    - decription "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation \n",
    "\n",
    "* Examine your models (coefficients, parameters, errors, etc...)\n",
    "\n",
    "* Compute and interpret your results in terms of accuracy, precision, recall, ROC etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues / Improvements\n",
    "1. Dataset is very small\n",
    "2. Use regularization / initialization\n",
    "3. Use cross-validaiton\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  References\n",
    "   - Academic (if any)\n",
    "   - Online (if any)\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "\n",
    "- If you use and/or adapt your code from existing projects, you must provide links and acknowldge the authors. Keep in mind that all documents in your projects and code will be check against the official plagiarism detection tool used by Penn State ([Turnitin](https://turnitin.psu.edu))\n",
    "\n",
    "> *This code is based on .... (if any)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
