{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88c062c",
   "metadata": {},
   "source": [
    "(Autoencoders)=\n",
    "# Autoencoders and anomaly detection\n",
    "\n",
    "This section explores the potential usage of autoencoders in the context of credit card fraud detection. \n",
    "\n",
    "## Definition and usage\n",
    "\n",
    "An autoencoder is a special type of deep learning architecture used to learn representations of data based solely on descriptive features. The representation, which is a transformation of the raw data, is learned with the objective to reconstruct the original data the most accurately. This representation learning strategy can be used for dimensionality reduction, denoising, or even generative applications. \n",
    "\n",
    "An autoencoder can be divided into two parts:\n",
    "* The **encoder part** that maps the input into the representation, also referred to as the \"code\" or the \"bottleneck\".\n",
    "* The **decoder that** maps the code to a reconstruction of the input.\n",
    "\n",
    "![Autoencoder](./images/autoencoder.png)\n",
    "\n",
    "The encoder and decoder can have complex architectures like recurrent neural networks when dealing with sequential data or convolutional neural networks when dealing with images. But in their simplest form, they are multi-layer feed-forward neural networks. The dimension of the code, which is also the input of the decoder, can be fixed arbitrarily. This dimension is generally chosen to be lower than the original input dimension to reduce the dimensionality and to learn underlying meta variables. The dimension of the output of the decoder is the same as the input of the encoder because its purpose is to reconstruct the input.  \n",
    "\n",
    "The architecture is generally trained end-to-end by optimizing the input reconstruction, i.e. by minimizing a loss that measures a difference between the model's output and the input. It can be trained with any unlabeled data. Note that when the autoencoder is \"deep\", i.e. there are intermediate layers $h_2$ and $h_2'$ respectively between the input $x$ and the bottleneck $h$ and between the bottleneck and the output $y$ (like in the figure above), one can train the layers successively instead of simultaneously. More precisely, one can first consider a submodel with only $x$, $h_2$ and $y$ and train it to reconstruct the input from the intermediate code $h_2$. Then, consider a second submodel with only $h_2$, $h$ and $h_2'$ and train it to reconstruct the intermediate code from the code $h$. Finally, fine-tune the whole model with $x$, $h_2$, $h$, $h_2'$ and $y$ to reconstruct the input.\n",
    "\n",
    "Autoencoders can be used as techniques for unsupervised or semi-supervised anomaly detection, which led them to be used multiple times for credit card fraud detection {cite}`an2015variational,zhou2017anomaly`. \n",
    "\n",
    "\n",
    "### Anomaly detection\n",
    "\n",
    "Although not detailed before, fraud detection can be performed with both supervised and unsupervised techniques {cite}`carcillo2019combining,veeramachaneni2016ai`, as it is a special instance of a broader problem referred to as anomaly detection or outlier detection. The latter generally includes techniques to identify items that are rare or differ significantly from the \"normal\" behavior, observable in the majority of the data. \n",
    "\n",
    "And one can easily see how a credit card fraud can be defined as an anomaly in transactions. These anomalies can be rare events or unexpected bursts in the activity of a single cardholder behavior, or specific patterns, not necessarily rare, in the global consumers' behavior. Rare events or outliers can be detected with unsupervised techniques that learn the normality and which are able to estimate discrepancy to this normality. But detection of other types of anomaly can require supervised techniques with proper training.\n",
    "\n",
    "Therefore, one can think of three types of anomaly detection techniques:\n",
    "\n",
    "* Supervised techniques that were widely explored in previous sections and chapters. These techniques require annotations on data that consist of two classes, \"normal\" (or \"genuine\") and \"abnormal\" (or \"fraud\"), and they learn to discriminate between those classes.\n",
    "* Unsupervised techniques that aim at detecting anomalies by modeling the majority behavior and considering it as \"normal\". Then they detect the \"abnormal\" or fraudulent behavior by searching for examples that do not fit well to the normal behavior.  \n",
    "* Semi-supervised techniques that are in between the two above cases and that can learn from both unlabeled and labeled data to detect fraudulent transactions. \n",
    "\n",
    "An autoencoder can be used to model the normal behavior of data and detect outliers using the reconstruction error as an indicator. In particular, one way to do so is to train it to globally reconstruct transactions in a dataset. The normal trend that is observed in the majority of transactions will be better approximated than rare events. Therefore, the reconstruction error of \"normal\" data will be lower than the reconstruction error of outliers.\n",
    "\n",
    "An autoencoder can therefore be considered as an unsupervised technique for fraud detection. In this section, we will implement and test it for both semi-supervised and unsupervised fraud detection. \n",
    "\n",
    "### Representation learning\n",
    "\n",
    "Other than unsupervised anomaly detection, an autoencoder can simply be used as a general representation learning method for credit card transaction data. In a more complex manner than PCA, an autoencoder will learn a transformation from the original feature space to a representation space with new variables that encodes all the useful information to reconstruct the original data. \n",
    "\n",
    "If the dimension of the code is chosen to be 2 or 3, one can visualize the transaction in the novel 2D/3D space. Otherwise, the code can also be used for other purposes, like:\n",
    "\n",
    "* Clustering: Clustering can be performed on the code instead of the original features. Groups learned from the clustering can be useful to characterize the types of behaviors of consumers or fraudsters.\n",
    "* Additional or replacement variables: The code can be used as replacement variables, or additional variables, to train any supervised learning model for credit card fraud detection. \n",
    "\n",
    "### Content of the section\n",
    "\n",
    "The following puts into practice the use of autoencoders for credit card fraud detection. It starts by defining the data structures for unlabeled transactions. It then implements and evaluates an autoencoder for unsupervised fraud detection. The autoencoder is then used to compute transaction representation for visualization and clustering. Finally, we explore a semi-supervised strategy for fraud detection.\n",
    "\n",
    "Let us dive into it by making all necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb3a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization: Load shared functions and simulated data \n",
    "\n",
    "import os\n",
    "\n",
    "# Get simulated data from Github repository\n",
    "if not os.path.exists(\"simulated-data-transformed\"):\n",
    "    !git clone https://github.com/Fraud-Detection-Handbook/simulated-data-transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776081cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/lib/python3/dist-packages (from seaborn) (1.21.5)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/lib/python3/dist-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/lib/python3/dist-packages (from seaborn) (1.3.5)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "%run shared_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bbcf33",
   "metadata": {},
   "source": [
    "This section reuses some \"deep learning\" specific material that was implemented in the previous section. It includes the evaluation function, the preparation of generators, the early-stopping strategy, the training loop, and so on. This material has been added to the `shared functions`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78099fc",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fc116",
   "metadata": {},
   "source": [
    "The same experimental setup as the previous section is used for our exploration, i.e. a fixed training and validation period, and the same features from the transformed simulated data (`simulated-data-transformed/data/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0046c348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  files\n",
      "CPU times: user 189 ms, sys: 338 ms, total: 528 ms\n",
      "Wall time: 528 ms\n",
      "919767 transactions loaded, containing 8195 fraudulent transactions\n"
     ]
    }
   ],
   "source": [
    "DIR_INPUT='simulated-data-transformed/data/' \n",
    "\n",
    "BEGIN_DATE = \"2018-06-11\"\n",
    "END_DATE = \"2018-09-14\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),transactions_df.TX_FRAUD.sum()))\n",
    "\n",
    "output_feature=\"TX_FRAUD\"\n",
    "\n",
    "input_features=['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_30DAY_WINDOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9304197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the starting day for the training period, and the deltas\n",
    "start_date_training = datetime.datetime.strptime(\"2018-07-25\", \"%Y-%m-%d\")\n",
    "delta_train=7\n",
    "delta_delay=7\n",
    "delta_test=7\n",
    "\n",
    "\n",
    "delta_valid = delta_test\n",
    "\n",
    "start_date_training_with_valid = start_date_training+datetime.timedelta(days=-(delta_delay+delta_valid))\n",
    "\n",
    "(train_df, valid_df)=get_train_test_set(transactions_df,start_date_training_with_valid,\n",
    "                                       delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
    "\n",
    "# By default, scales input data\n",
    "(train_df, valid_df)=scaleData(train_df, valid_df,input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc42cc3",
   "metadata": {},
   "source": [
    "## Autoencoder implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7017828",
   "metadata": {},
   "source": [
    "For the sake of consistency, the implementation of the autoencoders will be done with the `PyTorch` library. As usual, a seed will be used as follows to ensure reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1de369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\" \n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(\"Selected device is\",DEVICE)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0f7b9",
   "metadata": {},
   "source": [
    "Let us also convert our features and labels into torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "941ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(train_df[input_features].values)\n",
    "x_valid = torch.FloatTensor(valid_df[input_features].values)\n",
    "y_train = torch.FloatTensor(train_df[output_feature].values)\n",
    "y_valid = torch.FloatTensor(valid_df[output_feature].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecf5c0",
   "metadata": {},
   "source": [
    "The autoencoder has the same input as the baseline feed-forward neural network but a different output. Instead of the fraud/genuine label, its target will be the same as the input. Therefore, the experiments here will not rely on the `FraudDataset` defined before but on a new Dataset: `FraudDatasetUnsupervised`, which only receives the descriptive features of the transaction `x` and returns it as both input and output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd337728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDatasetUnsupervised(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x,output=True):\n",
    "        'Initialization'\n",
    "        self.x = x\n",
    "        self.output = output\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample index\n",
    "        item = self.x[index].to(DEVICE)\n",
    "        if self.output:\n",
    "            return item, item\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00123306",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = FraudDatasetUnsupervised(x_train)\n",
    "valid_set = FraudDatasetUnsupervised(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e54aab",
   "metadata": {},
   "source": [
    "This Dataset can also be turned into `DataLoaders` with the function `prepare_generators` from the shared functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bdf9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator,valid_generator = prepare_generators(training_set, valid_set, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb81604",
   "metadata": {},
   "source": [
    "The second and main element in our deep learning pipeline is the model/module. Since our data are tabular and each sample is a vector, we will resort to a regular feed-forward autoencoder. Its definition is very similar to our supervised feed-forward network for fraud detection, except that the output has as many neurons as the input, with linear activations, instead of a single neuron with sigmoid activation. An intermediate layer, before the representation layer, will also be considered such that the overall succession of layers with their dimensions (`input_dim`, `output_dim`) are the following:\n",
    "\n",
    "* A first input layer with ReLu activation (`input_size`, `intermediate_size`)\n",
    "* A second layer with ReLu activation (`intermediate_size`, `code_size`)\n",
    "* A third layer with ReLu activation (`code_size`, `intermediate_size`)\n",
    "* An output layer with linear activation (`intermediate_size`, `input_size`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6673afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, input_size, intermediate_size, code_size):\n",
    "            super(SimpleAutoencoder, self).__init__()\n",
    "            # parameters\n",
    "            self.input_size = input_size\n",
    "            self.intermediate_size = intermediate_size           \n",
    "            self.code_size  = code_size\n",
    "            \n",
    "            self.relu = torch.nn.ReLU()   \n",
    "            \n",
    "            #encoder\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.intermediate_size)\n",
    "            self.fc2 = torch.nn.Linear(self.intermediate_size, self.code_size)\n",
    "            \n",
    "            #decoder \n",
    "            self.fc3 = torch.nn.Linear(self.code_size, self.intermediate_size)            \n",
    "            self.fc4 = torch.nn.Linear(self.intermediate_size, self.input_size)\n",
    "            \n",
    "            \n",
    "        def forward(self, x):\n",
    "            \n",
    "            hidden = self.fc1(x)\n",
    "            hidden = self.relu(hidden)\n",
    "            \n",
    "            code = self.fc2(hidden)\n",
    "            code = self.relu(code)\n",
    " \n",
    "            hidden = self.fc3(code)\n",
    "            hidden = self.relu(hidden)\n",
    "            \n",
    "            output = self.fc4(hidden)\n",
    "            #linear activation in final layer)            \n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34893296",
   "metadata": {},
   "source": [
    "The third element of our pipeline is the optimization problem. The underlying machine learning problem is a regression here, where the predicted and expected outputs are real-valued variables. Therefore, the most adapted loss function is the mean squared error `torch.nn.MSELoss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee1cb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79a3c8",
   "metadata": {},
   "source": [
    "## Using the autoencoder for unsupervised fraud detection\n",
    "\n",
    "As explained in the introduction, the autoencoder's goal is to predict the input from the input. Therefore, one cannot directly use its prediction for fraud detection. Instead, the idea is to use its reconstruction error, i.e. the mean squared error (MSE) between the input and the output, as an indicator for fraud likelihood. The higher the error, the higher the risk score. Therefore, the reconstruction error can be considered as a predicted fraud risk score, and its relevance can be directly measured with any threshold-free metric.\n",
    "\n",
    "For that purpose, let us define a function `per_sample_mse` that will compute the MSE of a `model` for each sample provided by a `generator`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bddc6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sample_mse(model, generator):\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    batch_losses = []\n",
    "    \n",
    "    for x_batch, y_batch in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        loss_app = list(torch.mean(loss,axis=1).detach().cpu().numpy())\n",
    "        batch_losses.extend(loss_app)\n",
    "    \n",
    "    return batch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02da0d",
   "metadata": {},
   "source": [
    "Here is what happens when trying it on the validation samples with an untrained autoencoder. Let us use 100 neurons in the intermediate layer and 20 neurons in the representation layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c220fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "model = SimpleAutoencoder(x_train.shape[1], 100, 20).to(DEVICE)\n",
    "losses = per_sample_mse(model, valid_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b48057",
   "metadata": {},
   "source": [
    "Before training it, here are the loss values for the first five samples, and the overall average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acacae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6754841, 0.7914626, 1.1697073, 0.807015, 1.258897]\n",
      "0.9325166\n"
     ]
    }
   ],
   "source": [
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb7dd9",
   "metadata": {},
   "source": [
    "With random weights in its layers, the untrained autoencoder is rather bad at reconstruction. It has a squared error of 0.93 on average for the standardized transaction variables. \n",
    "\n",
    "Let us now train it and see how this evolves. Like in the previous section, the process is the following:\n",
    "* Prepare the generators.\n",
    "* Define the criterion.\n",
    "* Instantiate the model.\n",
    "* Perform several optimization loops (with an optimization technique like gradient descent with Adam) on the training data.\n",
    "* Stop optimization with early stopping using validation data.\n",
    "\n",
    "All of these steps are implemented in the shared function `training_loop` defined in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e363681",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "training_generator,valid_generator = prepare_generators(training_set, valid_set, batch_size = 64)\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(DEVICE)\n",
    "\n",
    "model = SimpleAutoencoder(len(input_features), 100,20).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ecae54f",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 7.351608883167862e-05\n",
      "valid loss: 6.194809032079275e-05\n",
      "New best score: 6.194809032079275e-05\n",
      "\n",
      "Epoch 1: train loss: 7.20266946397991e-05\n",
      "valid loss: 7.547278217545105e-05\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 2: train loss: 6.966246344338816e-05\n",
      "valid loss: 6.408207307368023e-05\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 3: train loss: 6.70805740434842e-05\n",
      "valid loss: 8.161097596468827e-05\n",
      "3  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model,training_execution_time,train_losses,valid_losses = training_loop(model,\n",
    "                                                                        training_generator,\n",
    "                                                                        valid_generator,\n",
    "                                                                        optimizer,\n",
    "                                                                        criterion,\n",
    "                                                                        max_epochs=500,\n",
    "                                                                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aef105d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.3156422e-05, 3.484918e-05, 5.5128607e-05, 5.2960782e-05, 4.2491967e-05]\n",
      "8.1655664e-05\n"
     ]
    }
   ],
   "source": [
    "losses = per_sample_mse(model, valid_generator)\n",
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "511a7542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1323, -0.6306,  2.1808, -0.3003,  0.1241, -1.6917,  0.5035, -1.6630,\n",
      "        -0.0482, -0.9810, -0.0816, -1.9895, -0.1231, -0.9719, -0.1436])\n",
      "tensor([-0.1322, -0.6274,  2.1892, -0.3020,  0.1262, -1.6840,  0.4980, -1.6700,\n",
      "        -0.0547, -0.9859, -0.0881, -1.9903, -0.1258, -0.9646, -0.1302],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(model(x_train[0].to(DEVICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756e74f",
   "metadata": {},
   "source": [
    "When trained, the autoencoder is much better at encoding/decoding a transaction. It now obtains a very low squared error (0.00008) on average for our standardized transaction variables. Moreover, the example above (with `x_train[0]`) illustrates how well the reconstructed transaction is similar to the input transaction. \n",
    "\n",
    "Now the remaining question is the following: are frauds less well reconstructed than genuine transactions such that reconstruction error can be used as an indicator of fraud risk?\n",
    "\n",
    "To answer, one can compute the average squared error for fraudulent and genuine transactions separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b4a3825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fraud reconstruction error: 0.0016030675\n",
      "Average genuine reconstruction error: 7.186729e-05\n"
     ]
    }
   ],
   "source": [
    "genuine_losses = np.array(losses)[y_valid.cpu().numpy() == 0]\n",
    "fraud_losses = np.array(losses)[y_valid.cpu().numpy() == 1]\n",
    "print(\"Average fraud reconstruction error:\", np.mean(fraud_losses))\n",
    "print(\"Average genuine reconstruction error:\", np.mean(genuine_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a0455",
   "metadata": {},
   "source": [
    "It appears that frauds are indeed less well reconstructed than genuine transactions, which is very encouraging. Let us now compute the AUC ROC, the average precision, and card precision@100 on the validation set by considering the reconstruction error as a predicted fraud score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cd8ece3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.832</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  Card Precision@100\n",
       "0    0.832              0.177               0.214"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=losses\n",
    "    \n",
    "performance_assessment(predictions_df, top_k_list=[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92fa32",
   "metadata": {},
   "source": [
    "Although less accurate than the supervised techniques covered before, this unsupervised method leads to encouraging results and is much more accurate than the random classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad4e6f",
   "metadata": {},
   "source": [
    "## Comparison with another unsupervised baseline: Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72dc3f6",
   "metadata": {},
   "source": [
    "The autoencoder has a very high AUC ROC without making any use of the labels during training. To contrast this result and as a sanity check, it is interesting to implement and test another popular unsupervised baseline.\n",
    "\n",
    "Isolation Forest is a state-of-the-art anomaly detection technique that relies on tree-based models. It computes, for each sample of data, an anomaly score that reflects how atypical the sample is. In order to calculate this score, the algorithm tries to isolate the sample from the rest of the dataset recursively: it chooses a random cutoff (pair feature-threshold), and evaluates if it allows the sample at hand to be isolated. If so, the algorithm stops. Otherwise, it adds another cutoff, and repeats the process until the sample is isolated from the rest. This recursive data partitioning can be represented as a decision tree and the number of cutoffs necessary to isolate a sample can be considered as the anomaly score. The lower the number of cutoffs (i.e. the easier it is to isolate the data point), the more likely the sample is to be an outlier.\n",
    "\n",
    "This algorithm is implemented in `sklearn` under the class `sklearn.ensemble.IsolationForest`. Let us train it on the training data and evaluate the anomaly score of the validation data. On the latter, the anomaly score of a sample is computed from the average depth of the leaves reached by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bd3087e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsolationForest(n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "anomalyclassifier = IsolationForest(random_state=SEED, n_estimators=10)\n",
    "anomalyclassifier.fit(train_df[input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe639cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  Card Precision@100\n",
       "0    0.808              0.164                0.19"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = valid_df\n",
    "predictions_df['predictions'] = -anomalyclassifier.score_samples(valid_df[input_features])\n",
    "    \n",
    "performance_assessment(predictions_df, top_k_list=[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2f436",
   "metadata": {},
   "source": [
    "We can see that this state-of-the-art unsupervised baseline provides performances that are close (slightly lower) to those of the autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa1d6a",
   "metadata": {},
   "source": [
    "## Transactions representation, visualization and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85393bcf",
   "metadata": {},
   "source": [
    "Additionally to its ability to detect anomalies, the autoencoder has other advantages, as was mentioned in the introduction. In particular, after training, one can use the encoder part alone to obtain representations of the transactions for visualization or clustering purposes. For that, let us train an autoencoder with a code dimension of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28b48779",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.7699023352982209\n",
      "valid loss: 0.5805507547542698\n",
      "New best score: 0.5805507547542698\n",
      "\n",
      "Epoch 1: train loss: 0.5995249636257584\n",
      "valid loss: 0.5214713629803371\n",
      "New best score: 0.5214713629803371\n",
      "\n",
      "Epoch 2: train loss: 0.5190003950212927\n",
      "valid loss: 0.4764063163044674\n",
      "New best score: 0.4764063163044674\n",
      "\n",
      "Epoch 3: train loss: 0.49128982279428335\n",
      "valid loss: 0.4646284825163461\n",
      "New best score: 0.4646284825163461\n",
      "\n",
      "Epoch 4: train loss: 0.48287232294251314\n",
      "valid loss: 0.4596673409469792\n",
      "New best score: 0.4596673409469792\n",
      "\n",
      "Epoch 5: train loss: 0.47741594514586955\n",
      "valid loss: 0.45498099115376917\n",
      "New best score: 0.45498099115376917\n",
      "\n",
      "Epoch 6: train loss: 0.4717030933493638\n",
      "valid loss: 0.4498160322995785\n",
      "New best score: 0.4498160322995785\n",
      "\n",
      "Epoch 7: train loss: 0.46371500573012386\n",
      "valid loss: 0.4434448770323738\n",
      "New best score: 0.4434448770323738\n",
      "\n",
      "Epoch 8: train loss: 0.45431923527220697\n",
      "valid loss: 0.4363583912289208\n",
      "New best score: 0.4363583912289208\n",
      "\n",
      "Epoch 9: train loss: 0.4461799049981465\n",
      "valid loss: 0.4312526768185402\n",
      "New best score: 0.4312526768185402\n",
      "\n",
      "Epoch 10: train loss: 0.4404792085318447\n",
      "valid loss: 0.427487354184109\n",
      "New best score: 0.427487354184109\n",
      "\n",
      "Epoch 11: train loss: 0.43640787212844107\n",
      "valid loss: 0.42365450904669005\n",
      "New best score: 0.42365450904669005\n",
      "\n",
      "Epoch 12: train loss: 0.43249696014488176\n",
      "valid loss: 0.4206346491158334\n",
      "New best score: 0.4206346491158334\n",
      "\n",
      "Epoch 13: train loss: 0.4289965175704792\n",
      "valid loss: 0.41779908133986216\n",
      "New best score: 0.41779908133986216\n",
      "\n",
      "Epoch 14: train loss: 0.4253041034788748\n",
      "valid loss: 0.4143874262851444\n",
      "New best score: 0.4143874262851444\n",
      "\n",
      "Epoch 15: train loss: 0.42184758941370487\n",
      "valid loss: 0.410997563633111\n",
      "New best score: 0.410997563633111\n",
      "\n",
      "Epoch 16: train loss: 0.41837240037567075\n",
      "valid loss: 0.40852362731766834\n",
      "New best score: 0.40852362731766834\n",
      "\n",
      "Epoch 17: train loss: 0.4153258840679894\n",
      "valid loss: 0.4052674088810311\n",
      "New best score: 0.4052674088810311\n",
      "\n",
      "Epoch 18: train loss: 0.41260066628456116\n",
      "valid loss: 0.40273019331074805\n",
      "New best score: 0.40273019331074805\n",
      "\n",
      "Epoch 19: train loss: 0.40989166389915727\n",
      "valid loss: 0.4004626906308971\n",
      "New best score: 0.4004626906308971\n",
      "\n",
      "Epoch 20: train loss: 0.40727586269264693\n",
      "valid loss: 0.39815321004455856\n",
      "New best score: 0.39815321004455856\n",
      "\n",
      "Epoch 21: train loss: 0.40482778223249244\n",
      "valid loss: 0.3958188724159543\n",
      "New best score: 0.3958188724159543\n",
      "\n",
      "Epoch 22: train loss: 0.4026163791664244\n",
      "valid loss: 0.39392687948675104\n",
      "New best score: 0.39392687948675104\n",
      "\n",
      "Epoch 23: train loss: 0.4003197448203486\n",
      "valid loss: 0.39160190602469314\n",
      "New best score: 0.39160190602469314\n",
      "\n",
      "Epoch 24: train loss: 0.3983498374089228\n",
      "valid loss: 0.3900078914367436\n",
      "New best score: 0.3900078914367436\n",
      "\n",
      "Epoch 25: train loss: 0.3963694312889307\n",
      "valid loss: 0.3883939188034808\n",
      "New best score: 0.3883939188034808\n",
      "\n",
      "Epoch 26: train loss: 0.3944833478382851\n",
      "valid loss: 0.38622697538039724\n",
      "New best score: 0.38622697538039724\n",
      "\n",
      "Epoch 27: train loss: 0.3926755192459883\n",
      "valid loss: 0.384613249015287\n",
      "New best score: 0.384613249015287\n",
      "\n",
      "Epoch 28: train loss: 0.3910558227713204\n",
      "valid loss: 0.3830019511486012\n",
      "New best score: 0.3830019511486012\n",
      "\n",
      "Epoch 29: train loss: 0.38936237666050744\n",
      "valid loss: 0.38188893718471945\n",
      "New best score: 0.38188893718471945\n",
      "\n",
      "Epoch 30: train loss: 0.3877749747915432\n",
      "valid loss: 0.38018131596468835\n",
      "New best score: 0.38018131596468835\n",
      "\n",
      "Epoch 31: train loss: 0.38632242116599874\n",
      "valid loss: 0.3786937392963086\n",
      "New best score: 0.3786937392963086\n",
      "\n",
      "Epoch 32: train loss: 0.38490201786416894\n",
      "valid loss: 0.3776660592829595\n",
      "New best score: 0.3776660592829595\n",
      "\n",
      "Epoch 33: train loss: 0.38361618677813053\n",
      "valid loss: 0.37621149437023643\n",
      "New best score: 0.37621149437023643\n",
      "\n",
      "Epoch 34: train loss: 0.38236364596888395\n",
      "valid loss: 0.3750504219141163\n",
      "New best score: 0.3750504219141163\n",
      "\n",
      "Epoch 35: train loss: 0.38111369055384203\n",
      "valid loss: 0.37394679587721175\n",
      "New best score: 0.37394679587721175\n",
      "\n",
      "Epoch 36: train loss: 0.3800881996209489\n",
      "valid loss: 0.37306789451283834\n",
      "New best score: 0.37306789451283834\n",
      "\n",
      "Epoch 37: train loss: 0.3790544288849056\n",
      "valid loss: 0.3720989319470411\n",
      "New best score: 0.3720989319470411\n",
      "\n",
      "Epoch 38: train loss: 0.3780935495294075\n",
      "valid loss: 0.3709165943124907\n",
      "New best score: 0.3709165943124907\n",
      "\n",
      "Epoch 39: train loss: 0.37705295181069276\n",
      "valid loss: 0.3700108962795122\n",
      "New best score: 0.3700108962795122\n",
      "\n",
      "Epoch 40: train loss: 0.3761343858836942\n",
      "valid loss: 0.36918121682164445\n",
      "New best score: 0.36918121682164445\n",
      "\n",
      "Epoch 41: train loss: 0.3752167188699569\n",
      "valid loss: 0.3684018599368184\n",
      "New best score: 0.3684018599368184\n",
      "\n",
      "Epoch 42: train loss: 0.3745139954365918\n",
      "valid loss: 0.3678767052830243\n",
      "New best score: 0.3678767052830243\n",
      "\n",
      "Epoch 43: train loss: 0.3737488055377344\n",
      "valid loss: 0.36691846438770087\n",
      "New best score: 0.36691846438770087\n",
      "\n",
      "Epoch 44: train loss: 0.37300557821248276\n",
      "valid loss: 0.36630158881997804\n",
      "New best score: 0.36630158881997804\n",
      "\n",
      "Epoch 45: train loss: 0.3722918787205424\n",
      "valid loss: 0.36567909303910096\n",
      "New best score: 0.36567909303910096\n",
      "\n",
      "Epoch 46: train loss: 0.3716777449983028\n",
      "valid loss: 0.36515180655841617\n",
      "New best score: 0.36515180655841617\n",
      "\n",
      "Epoch 47: train loss: 0.371118095106878\n",
      "valid loss: 0.36409606840766845\n",
      "New best score: 0.36409606840766845\n",
      "\n",
      "Epoch 48: train loss: 0.37048215756680725\n",
      "valid loss: 0.363797498303033\n",
      "New best score: 0.363797498303033\n",
      "\n",
      "Epoch 49: train loss: 0.36992904447004854\n",
      "valid loss: 0.363345361472479\n",
      "New best score: 0.363345361472479\n",
      "\n",
      "Epoch 50: train loss: 0.3693648828603114\n",
      "valid loss: 0.36301319608922866\n",
      "New best score: 0.36301319608922866\n",
      "\n",
      "Epoch 51: train loss: 0.36890383608938404\n",
      "valid loss: 0.36282277139809616\n",
      "New best score: 0.36282277139809616\n",
      "\n",
      "Epoch 52: train loss: 0.3683270781833401\n",
      "valid loss: 0.36216376173040254\n",
      "New best score: 0.36216376173040254\n",
      "\n",
      "Epoch 53: train loss: 0.3678824933776436\n",
      "valid loss: 0.36148370256840856\n",
      "New best score: 0.36148370256840856\n",
      "\n",
      "Epoch 54: train loss: 0.36754954921127964\n",
      "valid loss: 0.36115361419531816\n",
      "New best score: 0.36115361419531816\n",
      "\n",
      "Epoch 55: train loss: 0.3670186684475802\n",
      "valid loss: 0.3608519325002295\n",
      "New best score: 0.3608519325002295\n",
      "\n",
      "Epoch 56: train loss: 0.36668652734838525\n",
      "valid loss: 0.3601395249855323\n",
      "New best score: 0.3601395249855323\n",
      "\n",
      "Epoch 57: train loss: 0.366198788437971\n",
      "valid loss: 0.35974217483906146\n",
      "New best score: 0.35974217483906146\n",
      "\n",
      "Epoch 58: train loss: 0.3657854982076594\n",
      "valid loss: 0.3593637402266101\n",
      "New best score: 0.3593637402266101\n",
      "\n",
      "Epoch 59: train loss: 0.36543779528163817\n",
      "valid loss: 0.35899121883462687\n",
      "New best score: 0.35899121883462687\n",
      "\n",
      "Epoch 60: train loss: 0.36504147887001986\n",
      "valid loss: 0.35899576720644216\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 61: train loss: 0.364758145883252\n",
      "valid loss: 0.35845997683663183\n",
      "New best score: 0.35845997683663183\n",
      "\n",
      "Epoch 62: train loss: 0.3644656495086094\n",
      "valid loss: 0.3580141978674248\n",
      "New best score: 0.3580141978674248\n",
      "\n",
      "Epoch 63: train loss: 0.3641118088997117\n",
      "valid loss: 0.3578940583382799\n",
      "New best score: 0.3578940583382799\n",
      "\n",
      "Epoch 64: train loss: 0.3637962833075405\n",
      "valid loss: 0.3575790871362217\n",
      "New best score: 0.3575790871362217\n",
      "\n",
      "Epoch 65: train loss: 0.3634485864217605\n",
      "valid loss: 0.3573079299079916\n",
      "New best score: 0.3573079299079916\n",
      "\n",
      "Epoch 66: train loss: 0.3631804707686258\n",
      "valid loss: 0.3568690488247272\n",
      "New best score: 0.3568690488247272\n",
      "\n",
      "Epoch 67: train loss: 0.3628396309527567\n",
      "valid loss: 0.35633439700134467\n",
      "New best score: 0.35633439700134467\n",
      "\n",
      "Epoch 68: train loss: 0.362590016359125\n",
      "valid loss: 0.3561640426760814\n",
      "New best score: 0.3561640426760814\n",
      "\n",
      "Epoch 69: train loss: 0.3623346358993541\n",
      "valid loss: 0.3560951875207203\n",
      "New best score: 0.3560951875207203\n",
      "\n",
      "Epoch 70: train loss: 0.36204449444952247\n",
      "valid loss: 0.3558531054707824\n",
      "New best score: 0.3558531054707824\n",
      "\n",
      "Epoch 71: train loss: 0.36178889887738636\n",
      "valid loss: 0.35543597019760986\n",
      "New best score: 0.35543597019760986\n",
      "\n",
      "Epoch 72: train loss: 0.3615996041557757\n",
      "valid loss: 0.3552083831015832\n",
      "New best score: 0.3552083831015832\n",
      "\n",
      "Epoch 73: train loss: 0.3614193475440173\n",
      "valid loss: 0.35555801813393995\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 74: train loss: 0.36118108769219875\n",
      "valid loss: 0.3554440309929717\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 75: train loss: 0.3609315479747652\n",
      "valid loss: 0.3548522383788896\n",
      "New best score: 0.3548522383788896\n",
      "\n",
      "Epoch 76: train loss: 0.3606842270202892\n",
      "valid loss: 0.3547469674075236\n",
      "New best score: 0.3547469674075236\n",
      "\n",
      "Epoch 77: train loss: 0.36048453601778807\n",
      "valid loss: 0.35443484661031943\n",
      "New best score: 0.35443484661031943\n",
      "\n",
      "Epoch 78: train loss: 0.36028757739819256\n",
      "valid loss: 0.35401020146132817\n",
      "New best score: 0.35401020146132817\n",
      "\n",
      "Epoch 79: train loss: 0.3601006181415591\n",
      "valid loss: 0.3539821106879438\n",
      "New best score: 0.3539821106879438\n",
      "\n",
      "Epoch 80: train loss: 0.359948035550391\n",
      "valid loss: 0.3535718832999631\n",
      "New best score: 0.3535718832999631\n",
      "\n",
      "Epoch 81: train loss: 0.3595948295657083\n",
      "valid loss: 0.3535658503327865\n",
      "New best score: 0.3535658503327865\n",
      "\n",
      "Epoch 82: train loss: 0.35954706766865224\n",
      "valid loss: 0.35315490648394726\n",
      "New best score: 0.35315490648394726\n",
      "\n",
      "Epoch 83: train loss: 0.35930402539884154\n",
      "valid loss: 0.352991548099153\n",
      "New best score: 0.352991548099153\n",
      "\n",
      "Epoch 84: train loss: 0.3591336454077843\n",
      "valid loss: 0.352961752323505\n",
      "New best score: 0.352961752323505\n",
      "\n",
      "Epoch 85: train loss: 0.3590267771003807\n",
      "valid loss: 0.35313454247563264\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 86: train loss: 0.3588334535093654\n",
      "valid loss: 0.3527411215455154\n",
      "New best score: 0.3527411215455154\n",
      "\n",
      "Epoch 87: train loss: 0.35867820950812634\n",
      "valid loss: 0.3528638920334519\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 88: train loss: 0.3585490292048591\n",
      "valid loss: 0.35236634496456937\n",
      "New best score: 0.35236634496456937\n",
      "\n",
      "Epoch 89: train loss: 0.35836413001239187\n",
      "valid loss: 0.3519633968508309\n",
      "New best score: 0.3519633968508309\n",
      "\n",
      "Epoch 90: train loss: 0.35819637709890223\n",
      "valid loss: 0.3519061342288888\n",
      "New best score: 0.3519061342288888\n",
      "\n",
      "Epoch 91: train loss: 0.357988129638805\n",
      "valid loss: 0.35178797661932437\n",
      "New best score: 0.35178797661932437\n",
      "\n",
      "Epoch 92: train loss: 0.35786264901179426\n",
      "valid loss: 0.3516888407410168\n",
      "New best score: 0.3516888407410168\n",
      "\n",
      "Epoch 93: train loss: 0.35773589443522247\n",
      "valid loss: 0.35155985343977403\n",
      "New best score: 0.35155985343977403\n",
      "\n",
      "Epoch 94: train loss: 0.357428621066232\n",
      "valid loss: 0.3515522884215162\n",
      "New best score: 0.3515522884215162\n",
      "\n",
      "Epoch 95: train loss: 0.3574536673539455\n",
      "valid loss: 0.35143424936982454\n",
      "New best score: 0.35143424936982454\n",
      "\n",
      "Epoch 96: train loss: 0.3572946126157877\n",
      "valid loss: 0.3509989343217162\n",
      "New best score: 0.3509989343217162\n",
      "\n",
      "Epoch 97: train loss: 0.35714182149383805\n",
      "valid loss: 0.3511919470269823\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 98: train loss: 0.35690095371087693\n",
      "valid loss: 0.3510282319263031\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 99: train loss: 0.3568525843421998\n",
      "valid loss: 0.35051238499378246\n",
      "New best score: 0.35051238499378246\n",
      "\n",
      "Epoch 100: train loss: 0.35670698278834667\n",
      "valid loss: 0.351078713787058\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 101: train loss: 0.356550164278561\n",
      "valid loss: 0.35053075309333903\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 102: train loss: 0.35647697248148874\n",
      "valid loss: 0.3506819357474645\n",
      "3  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "training_generator,valid_generator = prepare_generators(training_set, valid_set, batch_size = 64)\n",
    "criterion = torch.nn.MSELoss().to(DEVICE)\n",
    "small_model = SimpleAutoencoder(len(input_features), 100,2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(small_model.parameters(), lr = 0.0001)\n",
    "\n",
    "small_model,training_execution_time,train_losses,valid_losses = training_loop(small_model,\n",
    "                                                                              training_generator,\n",
    "                                                                              valid_generator,\n",
    "                                                                              optimizer,\n",
    "                                                                              criterion,\n",
    "                                                                              max_epochs=500,\n",
    "                                                                              verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e704e5",
   "metadata": {},
   "source": [
    "Once trained, to obtain the transactions 2D-representation from the encoder part alone, the idea is to simply apply the first two layers of the Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0247badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_representation(x,model):\n",
    "    x_representation = model.fc1(x)\n",
    "    x_representation = model.relu(x_representation)\n",
    "    x_representation = model.fc2(x_representation)\n",
    "    x_representation = model.relu(x_representation)\n",
    "    return x_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1500b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_representation = []\n",
    "for x_batch, y_batch in training_generator: \n",
    "    x_train_representation.append(compute_representation(x_batch, small_model).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a11ecf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_representation = np.vstack(x_train_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbf3cb",
   "metadata": {},
   "source": [
    "After this process, the obtained representations of the training data are in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8b7d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66928, 15])\n",
      "(66928, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1040d1",
   "metadata": {},
   "source": [
    "Transactions can be now visualized on a plane (e.g. with different colors for frauds and genuine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bf8543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x70a23dce74c0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train_representation[:, 0], x_train_representation[:, 1], c=y_train.numpy(), s=50, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a46e1",
   "metadata": {},
   "source": [
    "It is also possible to apply a K-means clustering and vizualize the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8e6e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=SEED)\n",
    "kmeans.fit(x_train_representation)\n",
    "y_kmeans = kmeans.predict(x_train_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81d556e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x70a23d9712a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train_representation[:, 0], x_train_representation[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb9253",
   "metadata": {},
   "source": [
    "## Semi-supervised fraud detection\n",
    "\n",
    "Finally, the autoencoder can be used in a semi-supervised credit card fraud detection system {cite}`carcillo2019combining`. There are two main ways to do this:\n",
    "* `W1`: The most natural one is to keep the autoencoder as is and train it on all available labeled and unlabeled data. Then, to combine it with a supervised neural network trained only on labeled data. The combination can be done by aggregating the predicted score from the supervised model and the predicted score from the unsupervised model (`W1A`), or more elegantly by providing the unsupervised risk score from the autoencoder (reconstruction error) as an additional variable to the supervised model (`W1B`) as in {cite}`alazizi2020dual`.\n",
    "* `W2`: Another possibility is to change the architecture of the autoencoder into a hybrid semi-supervised model. More precisely, one can add, to the autoencoder, output neurons similar to those of the supervised neural network from the previous section, and additionally predict them from the code neurons. Therefore, the learned representation (code neurons) will be shared between the decoder network that aims at reconstructing the input and the prediction network that aims at predicting fraud. The first is trained on all samples and the latter is only trained on labeled samples. The intuition with this approach is similar to pre-training in natural language processing: learning a representation that embeds the underlying structure in the input data can help with solving supervised tasks. \n",
    "\n",
    "The following explores the `W1B` semi-supervised approach. But first, let us reevaluate here the baseline supervised model without the reconstruction error feature. `FraudDataset` and `SimpleFraudMLPWithDropout` are available in the shared functions and can be directly used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d236df19",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.10162742867403401\n",
      "valid loss: 0.03573565177639814\n",
      "New best score: 0.03573565177639814\n",
      "\n",
      "Epoch 1: train loss: 0.03880522794661826\n",
      "valid loss: 0.026402932935765858\n",
      "New best score: 0.026402932935765858\n",
      "\n",
      "Epoch 2: train loss: 0.031078890018542627\n",
      "valid loss: 0.02378855522905217\n",
      "New best score: 0.02378855522905217\n",
      "\n",
      "Epoch 3: train loss: 0.028793504271552993\n",
      "valid loss: 0.02269559715457179\n",
      "New best score: 0.02269559715457179\n",
      "\n",
      "Epoch 4: train loss: 0.027771245549525533\n",
      "valid loss: 0.022126405789705168\n",
      "New best score: 0.022126405789705168\n",
      "\n",
      "Epoch 5: train loss: 0.026878767684021924\n",
      "valid loss: 0.021896383678286127\n",
      "New best score: 0.021896383678286127\n",
      "\n",
      "Epoch 6: train loss: 0.026169354696495433\n",
      "valid loss: 0.02173952964920795\n",
      "New best score: 0.02173952964920795\n",
      "\n",
      "Epoch 7: train loss: 0.025671433107161728\n",
      "valid loss: 0.021000032945373222\n",
      "New best score: 0.021000032945373222\n",
      "\n",
      "Epoch 8: train loss: 0.024933777186159495\n",
      "valid loss: 0.021129449163283645\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 9: train loss: 0.0249975328226832\n",
      "valid loss: 0.02096466119273927\n",
      "New best score: 0.02096466119273927\n",
      "\n",
      "Epoch 10: train loss: 0.024326318861822283\n",
      "valid loss: 0.020618064747868828\n",
      "New best score: 0.020618064747868828\n",
      "\n",
      "Epoch 11: train loss: 0.024067563476574287\n",
      "valid loss: 0.02055384863188358\n",
      "New best score: 0.02055384863188358\n",
      "\n",
      "Epoch 12: train loss: 0.023930613619291417\n",
      "valid loss: 0.020029343267688987\n",
      "New best score: 0.020029343267688987\n",
      "\n",
      "Epoch 13: train loss: 0.023310760809504828\n",
      "valid loss: 0.02000891686496045\n",
      "New best score: 0.02000891686496045\n",
      "\n",
      "Epoch 14: train loss: 0.023393397197108645\n",
      "valid loss: 0.019778057898501636\n",
      "New best score: 0.019778057898501636\n",
      "\n",
      "Epoch 15: train loss: 0.023251842922864065\n",
      "valid loss: 0.01961277995965968\n",
      "New best score: 0.01961277995965968\n",
      "\n",
      "Epoch 16: train loss: 0.022855900806093492\n",
      "valid loss: 0.019497532883017768\n",
      "New best score: 0.019497532883017768\n",
      "\n",
      "Epoch 17: train loss: 0.022736568350467768\n",
      "valid loss: 0.019463124281543568\n",
      "New best score: 0.019463124281543568\n",
      "\n",
      "Epoch 18: train loss: 0.022446456162506505\n",
      "valid loss: 0.019271214333072793\n",
      "New best score: 0.019271214333072793\n",
      "\n",
      "Epoch 19: train loss: 0.022430788807750787\n",
      "valid loss: 0.019239478364276388\n",
      "New best score: 0.019239478364276388\n",
      "\n",
      "Epoch 20: train loss: 0.022228739047525398\n",
      "valid loss: 0.01908057054563278\n",
      "New best score: 0.01908057054563278\n",
      "\n",
      "Epoch 21: train loss: 0.022107687293298997\n",
      "valid loss: 0.019098266609181024\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 22: train loss: 0.022050552183551175\n",
      "valid loss: 0.019364916631962107\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 23: train loss: 0.021905896384990197\n",
      "valid loss: 0.018880290757035175\n",
      "New best score: 0.018880290757035175\n",
      "\n",
      "Epoch 24: train loss: 0.022007637326454256\n",
      "valid loss: 0.019101233521980218\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 25: train loss: 0.02185462507571541\n",
      "valid loss: 0.018792919955048403\n",
      "New best score: 0.018792919955048403\n",
      "\n",
      "Epoch 26: train loss: 0.021748732172894733\n",
      "valid loss: 0.01964420313002857\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 27: train loss: 0.021581903357997208\n",
      "valid loss: 0.01868849142958395\n",
      "New best score: 0.01868849142958395\n",
      "\n",
      "Epoch 28: train loss: 0.021206383293549784\n",
      "valid loss: 0.018618951422096193\n",
      "New best score: 0.018618951422096193\n",
      "\n",
      "Epoch 29: train loss: 0.02119581920386828\n",
      "valid loss: 0.019023919856058728\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 30: train loss: 0.021145932119736807\n",
      "valid loss: 0.01878707709027546\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 31: train loss: 0.021043205327696272\n",
      "valid loss: 0.018507413084547498\n",
      "New best score: 0.018507413084547498\n",
      "\n",
      "Epoch 32: train loss: 0.021028528965935718\n",
      "valid loss: 0.018481341556294776\n",
      "New best score: 0.018481341556294776\n",
      "\n",
      "Epoch 33: train loss: 0.020856277843541954\n",
      "valid loss: 0.018344308950561685\n",
      "New best score: 0.018344308950561685\n",
      "\n",
      "Epoch 34: train loss: 0.02082933506387042\n",
      "valid loss: 0.018535806504307223\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 35: train loss: 0.020760930473845002\n",
      "valid loss: 0.018355412500951174\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 36: train loss: 0.020845556747543147\n",
      "valid loss: 0.018244056229332024\n",
      "New best score: 0.018244056229332024\n",
      "\n",
      "Epoch 37: train loss: 0.020445872489968403\n",
      "valid loss: 0.018933365799873415\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 38: train loss: 0.02062339388378531\n",
      "valid loss: 0.01834845440831198\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 39: train loss: 0.0205844403894522\n",
      "valid loss: 0.0188435969269921\n",
      "3  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "training_set_supervised = FraudDataset(x_train.to(DEVICE), y_train.to(DEVICE))\n",
    "valid_set_supervised = FraudDataset(x_valid.to(DEVICE), y_valid.to(DEVICE))\n",
    "\n",
    "training_generator_supervised,valid_generator_supervised = prepare_generators(training_set_supervised,\n",
    "                                                                              valid_set_supervised,\n",
    "                                                                              batch_size=64)\n",
    "\n",
    "model_supervised = SimpleFraudMLPWithDropout(len(input_features), 1000, 0.2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model_supervised.parameters(), lr = 0.0001)\n",
    "criterion = torch.nn.BCELoss().to(DEVICE)\n",
    "\n",
    "model_supervised,training_execution_time,train_losses_dropout,valid_losses_dropout =\\\n",
    "    training_loop(model_supervised,\n",
    "                  training_generator_supervised,\n",
    "                  valid_generator_supervised,\n",
    "                  optimizer,\n",
    "                  criterion,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5c8093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.861</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  Card Precision@100\n",
       "0    0.861              0.647               0.277"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for x_batch, y_batch in valid_generator_supervised: \n",
    "    predictions.append(model_supervised(x_batch.to(DEVICE)).detach().cpu().numpy())\n",
    "\n",
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=np.vstack(predictions)\n",
    "    \n",
    "performance_assessment(predictions_df, top_k_list=[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10394218",
   "metadata": {},
   "source": [
    "Now, for the `W1B` semi-supervised approach, let us compute the reconstruction error of all transactions with our first autoencoder (stored in `model`) and add it as a new variable in `train_df` and `valid_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22d3ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_params = {'batch_size': 64,\n",
    "                 'num_workers': 0}\n",
    "    \n",
    "training_generator = torch.utils.data.DataLoader(training_set, **loader_params)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, **loader_params)\n",
    "\n",
    "train_reconstruction = per_sample_mse(model, training_generator)\n",
    "valid_reconstruction = per_sample_mse(model, valid_generator)\n",
    "\n",
    "train_df[\"reconstruction_error\"] = train_reconstruction\n",
    "valid_df[\"reconstruction_error\"] = valid_reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea7dd9",
   "metadata": {},
   "source": [
    "Then, we can reevaluate the supervised model with this extra variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "609af6d6",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.3245008781936953\n",
      "valid loss: 0.11662205916122009\n",
      "New best score: 0.11662205916122009\n",
      "\n",
      "Epoch 1: train loss: 0.08626749381658053\n",
      "valid loss: 0.050235452523322704\n",
      "New best score: 0.050235452523322704\n",
      "\n",
      "Epoch 2: train loss: 0.05462318545930358\n",
      "valid loss: 0.03678983381028221\n",
      "New best score: 0.03678983381028221\n",
      "\n",
      "Epoch 3: train loss: 0.045354362491279644\n",
      "valid loss: 0.031625012952142234\n",
      "New best score: 0.031625012952142234\n",
      "\n",
      "Epoch 4: train loss: 0.039736211981471224\n",
      "valid loss: 0.028533867803590546\n",
      "New best score: 0.028533867803590546\n",
      "\n",
      "Epoch 5: train loss: 0.03600632156161716\n",
      "valid loss: 0.026456877905010282\n",
      "New best score: 0.026456877905010282\n",
      "\n",
      "Epoch 6: train loss: 0.03363903012059016\n",
      "valid loss: 0.025115279116683074\n",
      "New best score: 0.025115279116683074\n",
      "\n",
      "Epoch 7: train loss: 0.03202404900245639\n",
      "valid loss: 0.024401237891701972\n",
      "New best score: 0.024401237891701972\n",
      "\n",
      "Epoch 8: train loss: 0.030768891625964648\n",
      "valid loss: 0.023709330617709728\n",
      "New best score: 0.023709330617709728\n",
      "\n",
      "Epoch 9: train loss: 0.029871157577072573\n",
      "valid loss: 0.023493107196065736\n",
      "New best score: 0.023493107196065736\n",
      "\n",
      "Epoch 10: train loss: 0.029569789335883116\n",
      "valid loss: 0.022988523661204782\n",
      "New best score: 0.022988523661204782\n",
      "\n",
      "Epoch 11: train loss: 0.028873421973192682\n",
      "valid loss: 0.022748412441238354\n",
      "New best score: 0.022748412441238354\n",
      "\n",
      "Epoch 12: train loss: 0.028341802562734764\n",
      "valid loss: 0.022515214663864077\n",
      "New best score: 0.022515214663864077\n",
      "\n",
      "Epoch 13: train loss: 0.028342050170333588\n",
      "valid loss: 0.022323308011028366\n",
      "New best score: 0.022323308011028366\n",
      "\n",
      "Epoch 14: train loss: 0.027967035156954256\n",
      "valid loss: 0.022162562861906293\n",
      "New best score: 0.022162562861906293\n",
      "\n",
      "Epoch 15: train loss: 0.02766981776394164\n",
      "valid loss: 0.022002750147602272\n",
      "New best score: 0.022002750147602272\n",
      "\n",
      "Epoch 16: train loss: 0.02757673605387927\n",
      "valid loss: 0.021849208917784316\n",
      "New best score: 0.021849208917784316\n",
      "\n",
      "Epoch 17: train loss: 0.027188794820277414\n",
      "valid loss: 0.02174542202802548\n",
      "New best score: 0.02174542202802548\n",
      "\n",
      "Epoch 18: train loss: 0.02686273712614982\n",
      "valid loss: 0.021643599502034892\n",
      "New best score: 0.021643599502034892\n",
      "\n",
      "Epoch 19: train loss: 0.026813590401395387\n",
      "valid loss: 0.021528060398130714\n",
      "New best score: 0.021528060398130714\n",
      "\n",
      "Epoch 20: train loss: 0.02704083643979206\n",
      "valid loss: 0.0214956170051038\n",
      "New best score: 0.0214956170051038\n",
      "\n",
      "Epoch 21: train loss: 0.026542368630102228\n",
      "valid loss: 0.02135520121566137\n",
      "New best score: 0.02135520121566137\n",
      "\n",
      "Epoch 22: train loss: 0.026390802910521156\n",
      "valid loss: 0.02126454722907272\n",
      "New best score: 0.02126454722907272\n",
      "\n",
      "Epoch 23: train loss: 0.026077186979040018\n",
      "valid loss: 0.02121463894622675\n",
      "New best score: 0.02121463894622675\n",
      "\n",
      "Epoch 24: train loss: 0.02581911200428663\n",
      "valid loss: 0.021041089193666446\n",
      "New best score: 0.021041089193666446\n",
      "\n",
      "Epoch 25: train loss: 0.02627791470539067\n",
      "valid loss: 0.02097927595862783\n",
      "New best score: 0.02097927595862783\n",
      "\n",
      "Epoch 26: train loss: 0.026127915991466657\n",
      "valid loss: 0.020888172916029808\n",
      "New best score: 0.020888172916029808\n",
      "\n",
      "Epoch 27: train loss: 0.025900807850468853\n",
      "valid loss: 0.020843728389489193\n",
      "New best score: 0.020843728389489193\n",
      "\n",
      "Epoch 28: train loss: 0.02569310541685856\n",
      "valid loss: 0.020748531431229428\n",
      "New best score: 0.020748531431229428\n",
      "\n",
      "Epoch 29: train loss: 0.02551448968162112\n",
      "valid loss: 0.020718631140958985\n",
      "New best score: 0.020718631140958985\n",
      "\n",
      "Epoch 30: train loss: 0.025239441464129785\n",
      "valid loss: 0.02061319018999125\n",
      "New best score: 0.02061319018999125\n",
      "\n",
      "Epoch 31: train loss: 0.025127840906392293\n",
      "valid loss: 0.020640792444989575\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 32: train loss: 0.025170398104235166\n",
      "valid loss: 0.020475286368250113\n",
      "New best score: 0.020475286368250113\n",
      "\n",
      "Epoch 33: train loss: 0.025183952624070857\n",
      "valid loss: 0.020418697411113018\n",
      "New best score: 0.020418697411113018\n",
      "\n",
      "Epoch 34: train loss: 0.025300677323065714\n",
      "valid loss: 0.020403656608023034\n",
      "New best score: 0.020403656608023034\n",
      "\n",
      "Epoch 35: train loss: 0.024830103280017397\n",
      "valid loss: 0.020284920431230768\n",
      "New best score: 0.020284920431230768\n",
      "\n",
      "Epoch 36: train loss: 0.024559190605426953\n",
      "valid loss: 0.02024191724698828\n",
      "New best score: 0.02024191724698828\n",
      "\n",
      "Epoch 37: train loss: 0.024717855663216864\n",
      "valid loss: 0.02012472948290998\n",
      "New best score: 0.02012472948290998\n",
      "\n",
      "Epoch 38: train loss: 0.02440387977110881\n",
      "valid loss: 0.020099012789576026\n",
      "New best score: 0.020099012789576026\n",
      "\n",
      "Epoch 39: train loss: 0.02472232144244058\n",
      "valid loss: 0.020086854079590672\n",
      "New best score: 0.020086854079590672\n",
      "\n",
      "Epoch 40: train loss: 0.024555766967483163\n",
      "valid loss: 0.019994302487504653\n",
      "New best score: 0.019994302487504653\n",
      "\n",
      "Epoch 41: train loss: 0.024166353714111907\n",
      "valid loss: 0.019965869318210403\n",
      "New best score: 0.019965869318210403\n",
      "\n",
      "Epoch 42: train loss: 0.024455780996344486\n",
      "valid loss: 0.01989088911904134\n",
      "New best score: 0.01989088911904134\n",
      "\n",
      "Epoch 43: train loss: 0.024243183072521057\n",
      "valid loss: 0.01982978222945125\n",
      "New best score: 0.01982978222945125\n",
      "\n",
      "Epoch 44: train loss: 0.023778501307960408\n",
      "valid loss: 0.019777366299121107\n",
      "New best score: 0.019777366299121107\n",
      "\n",
      "Epoch 45: train loss: 0.024046565680257107\n",
      "valid loss: 0.01978084870914374\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 46: train loss: 0.023983803367149618\n",
      "valid loss: 0.019689620529235306\n",
      "New best score: 0.019689620529235306\n",
      "\n",
      "Epoch 47: train loss: 0.023978895997899242\n",
      "valid loss: 0.019647505230911563\n",
      "New best score: 0.019647505230911563\n",
      "\n",
      "Epoch 48: train loss: 0.023812137591567098\n",
      "valid loss: 0.019611572138990577\n",
      "New best score: 0.019611572138990577\n",
      "\n",
      "Epoch 49: train loss: 0.02395904723672378\n",
      "valid loss: 0.01956746831454866\n",
      "New best score: 0.01956746831454866\n",
      "\n",
      "Epoch 50: train loss: 0.023714705458512782\n",
      "valid loss: 0.019630187885998622\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 51: train loss: 0.023219987343824033\n",
      "valid loss: 0.019537528951224854\n",
      "New best score: 0.019537528951224854\n",
      "\n",
      "Epoch 52: train loss: 0.023390244095128594\n",
      "valid loss: 0.019450321578792208\n",
      "New best score: 0.019450321578792208\n",
      "\n",
      "Epoch 53: train loss: 0.02333051290756615\n",
      "valid loss: 0.019389268805277967\n",
      "New best score: 0.019389268805277967\n",
      "\n",
      "Epoch 54: train loss: 0.02349304026802984\n",
      "valid loss: 0.019319080595115373\n",
      "New best score: 0.019319080595115373\n",
      "\n",
      "Epoch 55: train loss: 0.023489018436632588\n",
      "valid loss: 0.01934080402801943\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 56: train loss: 0.02326599685306061\n",
      "valid loss: 0.01924797346270923\n",
      "New best score: 0.01924797346270923\n",
      "\n",
      "Epoch 57: train loss: 0.023086270388998412\n",
      "valid loss: 0.01923319528126183\n",
      "New best score: 0.01923319528126183\n",
      "\n",
      "Epoch 58: train loss: 0.022994545524330886\n",
      "valid loss: 0.019259893998142354\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 59: train loss: 0.02299682347835834\n",
      "valid loss: 0.01910747012627296\n",
      "New best score: 0.01910747012627296\n",
      "\n",
      "Epoch 60: train loss: 0.02293265309010278\n",
      "valid loss: 0.01912479960751501\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 61: train loss: 0.022874201924173273\n",
      "valid loss: 0.01908286465344635\n",
      "New best score: 0.01908286465344635\n",
      "\n",
      "Epoch 62: train loss: 0.022883579581014342\n",
      "valid loss: 0.019023709187457143\n",
      "New best score: 0.019023709187457143\n",
      "\n",
      "Epoch 63: train loss: 0.022552970695433102\n",
      "valid loss: 0.01900861682510258\n",
      "New best score: 0.01900861682510258\n",
      "\n",
      "Epoch 64: train loss: 0.02290185722304298\n",
      "valid loss: 0.018919751521383154\n",
      "New best score: 0.018919751521383154\n",
      "\n",
      "Epoch 65: train loss: 0.022237511041412132\n",
      "valid loss: 0.018936518792958695\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 66: train loss: 0.0224765668513857\n",
      "valid loss: 0.0188365159140228\n",
      "New best score: 0.0188365159140228\n",
      "\n",
      "Epoch 67: train loss: 0.022505755006045416\n",
      "valid loss: 0.01884292140272268\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 68: train loss: 0.022571897592611904\n",
      "valid loss: 0.01880155499513093\n",
      "New best score: 0.01880155499513093\n",
      "\n",
      "Epoch 69: train loss: 0.02228572975508477\n",
      "valid loss: 0.018734690823332695\n",
      "New best score: 0.018734690823332695\n",
      "\n",
      "Epoch 70: train loss: 0.022412194719161466\n",
      "valid loss: 0.018725528703401974\n",
      "New best score: 0.018725528703401974\n",
      "\n",
      "Epoch 71: train loss: 0.022099832699337643\n",
      "valid loss: 0.018691658894175185\n",
      "New best score: 0.018691658894175185\n",
      "\n",
      "Epoch 72: train loss: 0.022258754139614735\n",
      "valid loss: 0.018754884264370823\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 73: train loss: 0.02263534135438046\n",
      "valid loss: 0.018694561797631855\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 74: train loss: 0.022091451830993696\n",
      "valid loss: 0.018590805308557435\n",
      "New best score: 0.018590805308557435\n",
      "\n",
      "Epoch 75: train loss: 0.022089719045800416\n",
      "valid loss: 0.01859218776149706\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 76: train loss: 0.0219967532201793\n",
      "valid loss: 0.01851542996117957\n",
      "New best score: 0.01851542996117957\n",
      "\n",
      "Epoch 77: train loss: 0.022149357172891133\n",
      "valid loss: 0.018515191289031358\n",
      "New best score: 0.018515191289031358\n",
      "\n",
      "Epoch 78: train loss: 0.022081862356363266\n",
      "valid loss: 0.018445001814840108\n",
      "New best score: 0.018445001814840108\n",
      "\n",
      "Epoch 79: train loss: 0.02205895189348124\n",
      "valid loss: 0.018433676820116048\n",
      "New best score: 0.018433676820116048\n",
      "\n",
      "Epoch 80: train loss: 0.021751448707512614\n",
      "valid loss: 0.018415803167475337\n",
      "New best score: 0.018415803167475337\n",
      "\n",
      "Epoch 81: train loss: 0.022140468084889984\n",
      "valid loss: 0.018502900250449452\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 82: train loss: 0.021965914356578357\n",
      "valid loss: 0.018381023700756168\n",
      "New best score: 0.018381023700756168\n",
      "\n",
      "Epoch 83: train loss: 0.022151830463815324\n",
      "valid loss: 0.01842671740454077\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 84: train loss: 0.021859681846224034\n",
      "valid loss: 0.018396422491266388\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 85: train loss: 0.02136607522340349\n",
      "valid loss: 0.01833893422182405\n",
      "New best score: 0.01833893422182405\n",
      "\n",
      "Epoch 86: train loss: 0.021509829373276983\n",
      "valid loss: 0.018249092375874885\n",
      "New best score: 0.018249092375874885\n",
      "\n",
      "Epoch 87: train loss: 0.021960712266248076\n",
      "valid loss: 0.01830329540226344\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 88: train loss: 0.021741690292658843\n",
      "valid loss: 0.018273073765627347\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 89: train loss: 0.02169192893841049\n",
      "valid loss: 0.01823332043265405\n",
      "New best score: 0.01823332043265405\n",
      "\n",
      "Epoch 90: train loss: 0.021708110392910124\n",
      "valid loss: 0.01825928057994327\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 91: train loss: 0.02185616356117676\n",
      "valid loss: 0.018209979526413594\n",
      "New best score: 0.018209979526413594\n",
      "\n",
      "Epoch 92: train loss: 0.02177859468512009\n",
      "valid loss: 0.018239955604330844\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 93: train loss: 0.02160099150887183\n",
      "valid loss: 0.018155546889823602\n",
      "New best score: 0.018155546889823602\n",
      "\n",
      "Epoch 94: train loss: 0.021527504544201204\n",
      "valid loss: 0.018190091264515585\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 95: train loss: 0.021674147826538422\n",
      "valid loss: 0.018165012851199695\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 96: train loss: 0.021242029102159447\n",
      "valid loss: 0.018102880319595702\n",
      "New best score: 0.018102880319595702\n",
      "\n",
      "Epoch 97: train loss: 0.02127737132698853\n",
      "valid loss: 0.018101166782625876\n",
      "New best score: 0.018101166782625876\n",
      "\n",
      "Epoch 98: train loss: 0.021420294640298192\n",
      "valid loss: 0.018135786327136468\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 99: train loss: 0.021323682591194434\n",
      "valid loss: 0.018110019210827808\n",
      "2  iterations since best score.\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "input_features_new = input_features + [\"reconstruction_error\"]\n",
    "\n",
    "# Rescale the reconstruction error\n",
    "(train_df, valid_df)=scaleData(train_df, valid_df, [\"reconstruction_error\"])\n",
    "\n",
    "x_train_new = torch.FloatTensor(train_df[input_features_new].values)\n",
    "x_valid_new = torch.FloatTensor(valid_df[input_features_new].values)\n",
    "\n",
    "training_set_supervised_new = FraudDataset(x_train_new.to(DEVICE), y_train.to(DEVICE))\n",
    "valid_set_supervised_new = FraudDataset(x_valid_new.to(DEVICE), y_valid.to(DEVICE))\n",
    "\n",
    "training_generator_supervised,valid_generator_supervised = prepare_generators(training_set_supervised_new,\n",
    "                                                                              valid_set_supervised_new,\n",
    "                                                                              batch_size=64)\n",
    "\n",
    "model_supervised = SimpleFraudMLPWithDropout(len(input_features_new), 100, 0.2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model_supervised.parameters(), lr = 0.0001)\n",
    "criterion = torch.nn.BCELoss().to(DEVICE)\n",
    "\n",
    "model_supervised,training_execution_time,train_losses_dropout,valid_losses_dropout = \\\n",
    "    training_loop(model_supervised,\n",
    "                  training_generator_supervised,\n",
    "                  valid_generator_supervised,\n",
    "                  optimizer,\n",
    "                  criterion,\n",
    "                  verbose=True)\n",
    "\n",
    "predictions = []\n",
    "for x_batch, y_batch in valid_generator_supervised: \n",
    "    predictions.append(model_supervised(x_batch).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22997be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.862</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  Card Precision@100\n",
       "0    0.862              0.653               0.274"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=np.vstack(predictions)\n",
    "    \n",
    "performance_assessment(predictions_df, top_k_list=[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bda478",
   "metadata": {},
   "source": [
    "The three metrics are very close, with or without the additional feature. They respectively went from 0.859, 0.646 and 0.28 to 0.861, 0.651 and 0.276. The conclusion is therefore mitigated and does not show a significant benefit from this semi-supervised modeling. Nevertheless, keep in mind that, in practice in a different setting, there can be a benefit, especially if the quantity of available unlabeled data is much larger than the quantity of labeled data. \n",
    "\n",
    "Also, note that there are several directions for improvement. For example, this semi-supervised technique can be pushed further by training two separate autoencoders, for each class, and by using both reconstruction errors as additional variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577e538",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779cb57",
   "metadata": {},
   "source": [
    "Autoencoders are part of the large deep learning models family. Their goal is to learn representations to reconstruct descriptive variables, so they have been widely used for unsupervised learning problems. Anomaly detection, and in particular fraud detection, can be tackled with unsupervised or semi-supervised techniques. \n",
    "\n",
    "In this section, we used the autoencoder, and in particular its reconstruction error, as an indicator for fraud risk. Used solely (unsupervised method), it detects data points that are away from the rest of the distribution, which allows detecting many frauds but also introduces a lot of false alerts (e.g. genuine transactions that have rare characteristics). Therefore, it obtains a decent AUC ROC but low precision-based metrics. Used as an extra variable in a supervised method (semi-supervised usage), it can allow boosting the performance in specific settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bab8eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_supervised.state_dict(), \"autoencoder_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
